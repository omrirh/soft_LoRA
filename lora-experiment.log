ubuntu@ip-10-0-0-14:~/LoRA$ docker logs 6ae43dd543e3

==========
== CUDA ==
==========

CUDA Version 11.3.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

README.md: 100%|███████████████████████████████████████████████████████████████████████████████████████| 35.3k/35.3k [00:00<00:00, 5.23MB/s]
train-00000-of-00001.parquet: 100%|████████████████████████████████████████████████████████████████████| 3.11M/3.11M [00:00<00:00, 56.2MB/s]
validation-00000-of-00001.parquet: 100%|███████████████████████████████████████████████████████████████| 72.8k/72.8k [00:00<00:00, 51.1MB/s]
test-00000-of-00001.parquet: 100%|███████████████████████████████████████████████████████████████████████| 148k/148k [00:00<00:00, 26.2MB/s]
Generating train split: 100%|█████████████████████████████████████████████████████████████| 67349/67349 [00:00<00:00, 1568988.06 examples/s]
Generating validation split: 100%|█████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 493380.96 examples/s]
Generating test split: 100%|█████████████████████████████████████████████████████████████████| 1821/1821 [00:00<00:00, 787080.34 examples/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<00:00, 31.3kB/s]
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 438kB/s]
vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 3.30MB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 6.47MB/s]
/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Map: 100%|███████████████████████████████████████████████████████████████████████████████████| 67349/67349 [00:16<00:00, 4034.03 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 4035.80 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1821/1821 [00:00<00:00, 4097.20 examples/s]
model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 440M/440M [00:02<00:00, 201MB/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|                                                                                                              | 0/6315 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
  2%|█���                                                                                                | 115/6315 [04:22<3:57:00,  2.29s/i{'loss': 0.558, 'grad_norm': 2.1533305644989014, 'learning_rate': 0.0004604117181314331, 'epoch': 0.24}
{'loss': 0.2831, 'grad_norm': 2.235999822616577, 'learning_rate': 0.0004208234362628662, 'epoch': 0.48}
 17%|█████████���██████▊                                                                                | 1094/6315 [41:48<3:19:10,  2.29s/i 20%|███���███████████████▋                                                                             | 1278/6315 [48:50<3:12:32,  2.29s/i{'loss': 0.2503, 'grad_norm': 1.5423110723495483, 'learning_rate': 0.00038123515439429927, 'epoch': 0.71}
 28%|██████████████████████���███▍                                                                    | 1757/6315 [1:07:09<2:54:11,  2.29s/i{'loss': 0.2292, 'grad_norm': 2.480733871459961, 'learning_rate': 0.0003416468725257324, 'epoch': 0.95}
{'eval_runtime': 25.1343, 'eval_samples_per_second': 34.694, 'eval_steps_per_second': 1.114, 'epoch': 1.0}
{'loss': 0.2062, 'grad_norm': 0.410023957490921, 'learning_rate': 0.0003020585906571655, 'epoch': 1.19}
 41%|███████���██████████████████████████████▋                                                        | 2575/6315 [1:38:52<2:22:56,  2.29s/i 43%|████████���███████████████████████████████▉                                                      | 2723/6315 [1:44:32<2:17:29,  2.30s/i{'loss': 0.1922, 'grad_norm': 1.5108500719070435, 'learning_rate': 0.0002624703087885986, 'epoch': 1.43}
 49%|████████████████████���█████████████████████████▏                                                | 3070/6315 [1:57:49<2:04:09,  2.30s/i 54%|██████████████████████████████████████████████████���▎                                           | 3412/6315 [2:10:56<1:51:18,  2.30s/i{'loss': 0.1867, 'grad_norm': 0.6172125935554504, 'learning_rate': 0.00022288202692003167, 'epoch': 1.66}
 56%|██���██████████████████████████████████████████████████▋                                         | 3566/6315 [2:16:51<1:45:40,  2.31s/i 59%|█████████████████████████████████████████████████���█████▌                                       | 3696/6315 [2:21:50<1:40:42,  2.31s/i 61%|████████████���████████████████████████████████████████████▌                                     | 3825/6315 [2:26:47<1:35:46,  2.31s/i{'loss': 0.1797, 'grad_norm': 0.9155499339103699, 'learning_rate': 0.00018329374505146476, 'epoch': 1.9}
 66%|██████████████████████████████████████████████���███████████████▉                                | 4185/6315 [2:40:37<1:21:50,  2.31s/i{'eval_runtime': 25.0166, 'eval_samples_per_second': 34.857, 'eval_steps_per_second': 1.119, 'epoch': 2.0}
 70%|████████████████████████████████���█████████████████████████████████▉                            | 4451/6315 [2:51:14<1:11:41,  2.31s/i{'loss': 0.1624, 'grad_norm': 0.40240776538848877, 'learning_rate': 0.00014370546318289787, 'epoch': 2.14}
 73%|████████████████████████████████████████████���████████████████████████▍                         | 4617/6315 [2:57:37<1:05:17,  2.31s/i 74%|████████████████████████████████████████████████████████████████���█████▎                        | 4675/6315 [2:59:50<1:03:02,  2.31s/i 76%|██████████████████████████████████████████████████████���██████████████████▌                       | 4790/6315 [3:04:15<58:38,  2.31s/i 78%|████████████████████████████████���██████████████████████████████████████████▎                     | 4903/6315 [3:08:36<54:17,  2.31s/i 79%|██████████████████████���█████████████████████████████████████████████████████▏                    | 4959/6315 [3:10:45<52:06,  2.31s/i{'loss': 0.1493, 'grad_norm': 1.1700359582901, 'learning_rate': 0.00010411718131433096, 'epoch': 2.38}
 83%|██████████████████████████████████████████████████���█████████████████████████████▉                | 5273/6315 [3:22:49<40:04,  2.31s/i 84%|███████████████████████████���█████████████████████████████████████████████████████▊               | 5327/6315 [3:24:54<37:52,  2.30s/i 85%|██████████████████████████████████████████████████████████████████████████████���███▋              | 5380/6315 [3:26:56<35:54,  2.30s/i{'loss': 0.1471, 'grad_norm': 7.079742431640625, 'learning_rate': 6.452889944576405e-05, 'epoch': 2.61}
 93%|█████████████████████████████████████████████████████████████████████���████████████████████       | 5862/6315 [3:45:28<17:25,  2.31s/i 94%|███████████████████████████████████████████���██████████████████████████████████████████████▊      | 5913/6315 [3:47:26<15:28,  2.31s/i{'loss': 0.1477, 'grad_norm': 1.3785922527313232, 'learning_rate': 2.494061757719715e-05, 'epoch': 2.85}
 96%|█████████���██████████████████████████████████████████████████████████████████████████████████▉    | 6050/6315 [3:52:42<10:11,  2.31s/i 97%|█████████████████████████████████████████████████████████████████████���████████████████████████▍  | 6149/6315 [3:56:30<06:23,  2.31s/i 98%|██████████���████████████████████████████████████████████████████████████████████████████████████▏ | 6199/6315 [3:58:26<04:27,  2.31s/i 99%|████████████████████████████████���██████████████████████████████████████████████████████████████▉ | 6248/6315 [4:00:19<02:34,  2.31s/i100%|█████████████████████████████���██████████████████████████████████████████████████████████████████▋| 6297/6315 [4:02:12<00:41,  2.31s/i{'eval_runtime': 25.2218, 'eval_samples_per_second': 34.573, 'eval_steps_per_second': 1.11, 'epoch': 3.0}
{'train_runtime': 14598.6323, 'train_samples_per_second': 13.84, 'train_steps_per_second': 0.433, 'train_loss': 0.220162666061051, 'epoch': 3.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 6315/6315 [4:03:18<00:00,  2.31s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:25<00:00,  1.11it/s]
Evaluation results: {'eval_runtime': 25.1951, 'eval_samples_per_second': 34.61, 'eval_steps_per_second': 1.111, 'epoch': 3.0}